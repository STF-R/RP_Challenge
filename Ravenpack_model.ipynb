{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Presentation of the challenge](#1) <br>\n",
    "- [1.1 - The RavenPack Data Science Challenge](#1.1) <br>\n",
    "- [1.2 - Overview of the approach](#1.2)<br><br>\n",
    "\n",
    "[2. Collect & transform data](#2) <br>\n",
    "- [2.1 - Connection à SQL Server](#2.1) <br>\n",
    "- [2.2 - Mise au format des données](#2.2)<br>\n",
    "- [](#2.3)<br>\n",
    "- [](#2.4)<br>\n",
    "- [](#2.5)<br>\n",
    "\n",
    "[3. Descriptive analysis / Statistical inferences](#3) <br>\n",
    "- [](#3.1)<br>\n",
    "- [](#3.2)<br>\n",
    "- [](#3.3)<br>\n",
    "- [](#3.4)<br>\n",
    "\n",
    "[4. Preprocess the data](#4) <br>\n",
    "- [4.1 - Clustering](#4.1)<br>\n",
    "- [4.2 - Création de la target (y)](#4.2)<br>\n",
    "- [4.3 - Valeurs aberrantes](#4.3)<br>\n",
    "- [4.4 - One-hot-encoding](#4.4)<br><br>\n",
    "\n",
    "[5. Create features](#5) <br>\n",
    "- [5.1 - Dataset du modèle 1](#5.1)<br>\n",
    "- [](#5.2)<br>\n",
    "- [](#5.3)<br>\n",
    "- [](#5.4)<br>\n",
    "\n",
    "[6. Select a ML algo](#6) <br>\n",
    "- [6.1 - Dataset du modèle 1](#6.1)<br>\n",
    "- [](#6.2)<br>\n",
    "- [](#6.3)<br>\n",
    "- [](#6.4)<br>\n",
    "\n",
    "[7. Backtest on unseen data](#7) <br>\n",
    "- [7.1 - Dataset du modèle 1](#7.1)<br>\n",
    "- [](#7.2)<br>\n",
    "- [](#7.3)<br>\n",
    "- [](#7.4)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <a id =4> </a> **4. Preprocess the data**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy: We implemented a trading strategy in simulation as follows: On the first trading day of each week, we compute a forecast for each member of the S&P 500. We assess each decile (groups of 50) of stocks ranked from highest forecast to lowest, as follows: We enter an equally-weighted long position in each group of 50 stocks. Positions are held one week, and then rebalanced.\n",
    "\n",
    "Reversion\n",
    "Momentum\n",
    "Seasonality\n",
    "Lead-lag\n",
    "Learning...\n",
    "\n",
    "\n",
    "Events: \n",
    "=> entity detection: companies, people, products, commo, fx, orga...\n",
    "=> envent detection: 5 broader topics: business, economic, societal, political, environmental\n",
    "- Event relevance \n",
    "- Novel event (first time you see a particuliar event since a certain time)\n",
    "- Event buzz (abnormal news volume)\n",
    "- Sentiment scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id =4.1> </a>4.1 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "# from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n order to cluster our series with k-means, the essential thing to do is, as we do it with som, removing our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point. Another important thing to do is, selecting the distance metric. In the k-means algorithm, people usually use the euclidean distance but as we've seen in DBA, it is not effective in our case. So, we will be using Dynamic Time Warping (DTW) instead of euclidean distance and you can see why we are doing this in the following images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Time Warping Distance Metric for Time Series But first, why is the common Euclidean distance metric is unsuitable for time series? In short, it is invariant to time shifts, ignoring the time dimension of the data. If two time series are highly correlated, but one is shifted by even one time step, Euclidean distance would erroneously measure them as further apart. Instead, it is better to use dynamic time warping (DTW) to compare series. DTW is a technique to measure similarity between two temporal sequences that do not align exactly in time, speed, or length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series_0 = df.loc[:,['DATE', 'RP_ENTITY_ID', 'T0_RETURN']].pivot_table(index='DATE',columns='RP_ENTITY_ID',values='T0_RETURN')\n",
    "# time_series_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tslearn.metrics import dtw\n",
    "# A1, A2 = '619882','9196A2'#'50070E'#''#'507AE7' #\n",
    "\n",
    "# x = time_series_0.loc[:,A1]\n",
    "# y =  time_series_0.loc[:,A2]\n",
    "# df_xy = pd.merge(x, y, left_index=True, right_index=True).dropna()\n",
    "# df_xy = df_xy.loc[(df_xy.index.year>=2009)&(df_xy.index.year<=2009),:]\n",
    "# # df_xy = df_xy.resample(freq).mean()\n",
    "# x=df_xy.iloc[:,0]\n",
    "# y=df_xy.iloc[:,1]\n",
    "\n",
    "# dtw_score = dtw(x, y)\n",
    "# dtw_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyts\n",
    "# !conda install --ignore-installed llvmlite\n",
    "# !conda install numba==0.53.0\n",
    "# !pip install dtw-python\n",
    "# from dtw import *\n",
    "# # ?dtw\n",
    "# dd = dtw(\n",
    "#     x,\n",
    "#     y,\n",
    "#     dist_method='euclidean',\n",
    "#     step_pattern='symmetric2',\n",
    "#     window_type=\"sakoechiba\",\n",
    "#     window_args={'window_size':1},\n",
    "#     keep_internals=False,\n",
    "#     distance_only=False,\n",
    "#     open_end=False,\n",
    "#     open_begin=False,\n",
    "# )\n",
    "\n",
    "\n",
    "# # from pyts.metrics import dtw\n",
    "# # dtw(x, y, method='sakoechiba', options={'window_size': 0.5})\n",
    "\n",
    "# dd.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Symmetric: alignment follows the low-distance marked path\n",
    "\n",
    "# plt.plot(ds.index1,ds.index2)             # doctest: +SKIP\n",
    "\n",
    "# Asymmetric: visiting 1 is required twice\n",
    "\n",
    "# plt.plot(dd.index1,dd.index2,'ro')\n",
    "# df_xy.iloc[:,0:2].corr().iloc[0,1]\n",
    "\n",
    "# from scipy import stats\n",
    "# spear_corr = stats.spearmanr(df_xy.iloc[:,0:2])\n",
    "# spear_corr[0]\n",
    "\n",
    "# df_xy.iloc[:,0:2].plot()\n",
    "\n",
    "# df_xy['PRICE_A1'] = 100*(1 + df_xy.loc[:,A1]).cumprod()\n",
    "# df_xy['PRICE_A2'] = 100*(1 + df_xy.loc[:,A2]).cumprod()\n",
    "# df_xy.iloc[:,2:4].plot()\n",
    "\n",
    "# %matplotlib inline\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # df2 = df.query(\"RP_ENTITY_ID==@A1 or RP_ENTITY_ID==@A2\")#['DATE'].loc['2005-01-03':, :]\n",
    "# # df2 = df2[df2['DATE']>=beg]\n",
    "# df_xy['PRICE_A1'] = 100*(1 + df_xy.loc[:,A1]).cumprod()\n",
    "# df_xy['PRICE_A2'] = 100*(1 + df_xy.loc[:,A2]).cumprod()\n",
    "# #     df2 = df2.loc[:,['DATE','PRICE', 'RP_ENTITY_ID']]\n",
    "# #     df2 = df2.set_index('DATE')\n",
    "# #     df2 = df2.resample(freq).mean()\n",
    "    \n",
    "# fig.add_traces(go.Scatter(x=df_xy.index, y=df_xy.PRICE_A1, mode='lines', name = A1))\n",
    "# fig.add_traces(go.Scatter(x=df_xy.index, y=df_xy.PRICE_A2, mode='lines', name = A2))\n",
    "# fig.update_yaxes(title_text=\"y-axis in logarithmic scale\", type=\"log\")\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# from IPython.display import display\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <a id =5> </a> **5. Create features**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id =5.1> </a> 5.1 Check if the data is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from numpy import log\n",
    "\n",
    "\n",
    "\n",
    "list_asset = df.RP_ENTITY_ID.value_counts().loc[df.RP_ENTITY_ID.value_counts()>10000].index\n",
    "list_asset\n",
    "\n",
    "\n",
    "dg = df.loc[df['RP_ENTITY_ID'].isin(list_asset),'T0_RETURN'].dropna()\n",
    "result = adfuller(dg)\n",
    "print('p-value: %f' % result[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is below 0.05, the data can be assumed to be stationary hence we can proceed with the data without any transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#  <a id =6> </a> **6. Select a ML algo**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_dico\n",
    "\n",
    "{k: v for k, v in sorted(mean_score_dico.items(), key=lambda item: item[1])}\n",
    "\n",
    "mlist = [v[0] for v in mean_score_dico.values()]\n",
    "print('mean:', round(statistics.mean(mlist),4), '--- median:', round(np.quantile(mlist,0.5),4), '--- min:', round(min(mlist),2), '--- max:', round(max(mlist),2))\n",
    "\n",
    "nblist = [v[3] for v in mean_score_dico.values()]\n",
    "print('mean:', round(statistics.mean(nblist),4), '--- median:', round(np.quantile(nblist,0.5),4), '--- min:', round(min(nblist),2), '--- max:', round(max(nblist),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "###########################################################  LOG REG   ###################################################################### GOOD !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "# ceiling = 10000\n",
    "# floor = 756\n",
    "# list_asset = df.RP_ENTITY_ID.value_counts().loc[(df.RP_ENTITY_ID.value_counts()<ceiling)&(df.RP_ENTITY_ID.value_counts()>floor)].index\n",
    "# ret = 0.02\n",
    "# mask = (df['T1_RETURN']<=-ret) | (df['T1_RETURN']>=ret)\n",
    "\n",
    "mean_score_dico = {}\n",
    "for asset in list_asset:\n",
    "    print(asset)\n",
    "    dg = df_track_perf#.loc[mask].copy()\n",
    "    dg= dg.sort_values(['DATE','RP_ENTITY_ID'], ignore_index = True)\n",
    "    dg = dg.loc[dg['RP_ENTITY_ID']==asset, :]\n",
    "    dg['T1_RETURN_log'] = np.sign(dg['T1_RETURN_log'])\n",
    "    target_cols = ['T1_RETURN_log']\n",
    "    non_target_cols = list(set(dg.columns) - set(target_cols + ['RP_ENTITY_ID'] + ['DATE']))\n",
    "\n",
    "#     X_all = .iloc[:,2:-1]\n",
    "# y = df_track_perf.T1_RETURN_log\n",
    "#     X = dg.loc[:,non_target_cols]\n",
    "#     y = dg.loc[:,target_cols]\n",
    "    dh=dg.loc[:,['GLOBAL_ALL', 'T0_RETURN_log', 'T1_RETURN_log']].dropna()\n",
    "    X = dh.loc[:,['GLOBAL_ALL', 'T0_RETURN_log']]\n",
    "    y = dh.loc[:,'T1_RETURN_log']\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    print(tscv)\n",
    "\n",
    "    mlist=[]\n",
    "    count=0\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        start_time = time.time()\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        rfr = LogisticRegression(random_state=0)\n",
    "        try:\n",
    "            rfr.fit(X_train, y_train.values.ravel())\n",
    "            scor=rfr.score(X_test, y_test.values.ravel())\n",
    "            mlist.append(scor)\n",
    "            print(scor)\n",
    "            elapsed_time = time.time() - start_time\n",
    "        except ValueError:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        count+=1\n",
    "        print(f\"Elapsed time to compute the tab_values{count:.0f}: {elapsed_time:.3f} seconds\")\n",
    "        print(len(X_train), len(X_test))\n",
    "    mean_score=statistics.mean(mlist)\n",
    "    std_score=statistics.stdev(mlist)\n",
    "    median_score=statistics.median(mlist)\n",
    "    print('mean=',mean_score)\n",
    "    mean_score_dico[asset] = mean_score,std_score,median_score,len(X_test)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "##################################################   RF   ####################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "mean_score_dico = {}\n",
    "\n",
    "for asset in list_asset:\n",
    "    print(asset)\n",
    "    dg = df_track_perf#.loc[mask].copy()\n",
    "    dg= dg.sort_values(['DATE','RP_ENTITY_ID'], ignore_index = True)\n",
    "    dg = dg.loc[dg['RP_ENTITY_ID']==asset, :]\n",
    "    dg['T1_RETURN_log'] = np.sign(dg['T1_RETURN_log'])\n",
    "    target_cols = ['T1_RETURN_log']\n",
    "    non_target_cols = list(set(dg.columns) - set(target_cols + ['RP_ENTITY_ID'] + ['DATE']))\n",
    "\n",
    "\n",
    "    dh=dg.loc[:,['GLOBAL_ALL', 'T0_RETURN_log', 'T1_RETURN_log']]#.dropna()\n",
    "    X = dh.loc[:,['GLOBAL_ALL', 'T0_RETURN_log']]\n",
    "    y = dh.loc[:,'T1_RETURN_log']\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    print(tscv)\n",
    "\n",
    "    mlist=[]\n",
    "    count=0\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        start_time = time.time()\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        rfr = RandomForestClassifier(random_state=0)\n",
    "        try:\n",
    "            rfr.fit(X_train, y_train.values.ravel())\n",
    "            scor=rfr.score(X_test, y_test.values.ravel())\n",
    "            mlist.append(scor)\n",
    "            print(scor)\n",
    "            elapsed_time = time.time() - start_time\n",
    "        except ValueError:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        count+=1\n",
    "        print(f\"Elapsed time to compute the tab_values{count:.0f}: {elapsed_time:.3f} seconds\")\n",
    "        print(len(X_train), len(X_test))\n",
    "    mean_score=statistics.mean(mlist)\n",
    "    std_score=statistics.stdev(mlist)\n",
    "    median_score=statistics.median(mlist)\n",
    "    print('mean=',mean_score)\n",
    "    mean_score_dico[asset] = mean_score,std_score,median_score,len(X_test)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#################################     LIST OF ASSETS 'GLOBAL_ALL'    ########################################################################\n",
    "##################################################   RF   ###################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "mean_score_dico = {}\n",
    "asset =['EF5BED', '40B903', '2E61CC', '061856', '034B61', '73C521', '96B4C5', '2667B6', '9CA619', 'FF6644']\n",
    "\n",
    "dg = df_track_perf#.loc[mask].copy()\n",
    "dg= dg.sort_values(['DATE','RP_ENTITY_ID'], ignore_index = True)\n",
    "dg = dg.loc[dg['RP_ENTITY_ID'].isin(asset), :]\n",
    "dg['T1_RETURN_log'] = np.sign(dg['T1_RETURN_log'])\n",
    "target_cols = ['T1_RETURN_log']\n",
    "non_target_cols = list(set(dg.columns) - set(target_cols + ['RP_ENTITY_ID'] + ['DATE']))\n",
    "\n",
    "\n",
    "dh=dg.loc[:,['GLOBAL_ALL', 'T0_RETURN_log', 'T1_RETURN_log']]#.dropna()\n",
    "X = dh.loc[:,['GLOBAL_ALL', 'T0_RETURN_log']]\n",
    "y = dh.loc[:,'T1_RETURN_log']\n",
    "tscv = TimeSeriesSplit(n_splits=7)\n",
    "print(tscv)\n",
    "\n",
    "mlist=[]\n",
    "count=0\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    start_time = time.time()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    rfr = RandomForestClassifier(random_state=3)\n",
    "    try:\n",
    "        rfr.fit(X_train, y_train.values.ravel())\n",
    "        scor=rfr.score(X_test, y_test.values.ravel())\n",
    "        mlist.append(scor)\n",
    "        print(scor)\n",
    "        elapsed_time = time.time() - start_time\n",
    "    except ValueError:\n",
    "                pass\n",
    "\n",
    "\n",
    "    count+=1\n",
    "    print(f\"Elapsed time to compute the tab_values{count:.0f}: {elapsed_time:.3f} seconds\")\n",
    "    print(len(X_train), len(X_test))\n",
    "mean_score=statistics.mean(mlist)\n",
    "std_score=statistics.stdev(mlist)\n",
    "median_score=statistics.median(mlist)\n",
    "print('mean=',mean_score)\n",
    "mean_score_dico[str(asset)] = mean_score,std_score,median_score,len(X_test)\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "###################   LIST OF ASSETS  ['GROUP_A_ALL', 'GROUP_E_ALL', 'T0_RETURN', 'T1_RETURN'] ##############################################\n",
    "##################################################   RF   ###################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "#############################################################################################################################################\n",
    "mean_score_dico = {}\n",
    "asset =['EF5BED', '40B903', '2E61CC', '061856', '034B61', '73C521', '96B4C5', '2667B6', '9CA619', 'FF6644']\n",
    "\n",
    "dg = df_track_perf#.loc[mask].copy()\n",
    "dg= dg.sort_values(['DATE','RP_ENTITY_ID'], ignore_index = True)\n",
    "dg = dg.loc[dg['RP_ENTITY_ID'].isin(asset), :]\n",
    "dg['T1_RETURN_log'] = np.sign(dg['T1_RETURN_log'])\n",
    "target_cols = ['T1_RETURN_log']\n",
    "non_target_cols = list(set(dg.columns) - set(target_cols + ['RP_ENTITY_ID'] + ['DATE']))\n",
    "\n",
    "\n",
    "dh=dg.loc[:,['GROUP_A_ALL', 'GROUP_E_ALL', 'T0_RETURN_log', 'T1_RETURN_log']].dropna()\n",
    "X = dh.loc[:,['GROUP_A_ALL', 'GROUP_E_ALL', 'T0_RETURN_log']]\n",
    "y = dh.loc[:,'T1_RETURN_log']\n",
    "tscv = TimeSeriesSplit(n_splits=7)\n",
    "print(tscv)\n",
    "\n",
    "mlist=[]\n",
    "count=0\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    start_time = time.time()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    rfr = RandomForestClassifier(random_state=3)\n",
    "    try:\n",
    "        rfr.fit(X_train, y_train.values.ravel())\n",
    "        scor=rfr.score(X_test, y_test.values.ravel())\n",
    "        mlist.append(scor)\n",
    "        print(scor)\n",
    "        elapsed_time = time.time() - start_time\n",
    "    except ValueError:\n",
    "                pass\n",
    "\n",
    "\n",
    "    count+=1\n",
    "    print(f\"Elapsed time to compute the tab_values{count:.0f}: {elapsed_time:.3f} seconds\")\n",
    "    print(len(X_train), len(X_test))\n",
    "mean_score=statistics.mean(mlist)\n",
    "std_score=statistics.stdev(mlist)\n",
    "median_score=statistics.median(mlist)\n",
    "print('mean=',mean_score)\n",
    "mean_score_dico[str(asset)] = mean_score,std_score,median_score,len(X_test)\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(rf.feature_importances_,index=X_test.columns).sort_values(ascending=False)\n",
    "feature_imp.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# <a id =7> </a> **7. Backtest on unseen data**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
